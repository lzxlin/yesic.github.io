<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Tech-Port</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2019-04-28T13:47:18.000Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>yesic</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Hello World</title>
    <link href="http://yoursite.com/2019/04/28/hello-world/"/>
    <id>http://yoursite.com/2019/04/28/hello-world/</id>
    <published>2019-04-28T13:47:17.960Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.<br><a id="more"></a></p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;documentation&lt;/a&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;a href=&quot;https://hexo.io/docs/troubleshooting.html&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;troubleshooting&lt;/a&gt; or you can ask me on &lt;a href=&quot;https://github.com/hexojs/hexo/issues&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;GitHub&lt;/a&gt;.&lt;br&gt;
    
    </summary>
    
      <category term="test" scheme="http://yoursite.com/categories/test/"/>
    
    
  </entry>
  
  <entry>
    <title>[CVPR 2018]Deep Layer Aggregation</title>
    <link href="http://yoursite.com/2018/07/01/CVPR-2018-Deep-Layer-Aggregation/"/>
    <id>http://yoursite.com/2018/07/01/CVPR-2018-Deep-Layer-Aggregation/</id>
    <published>2018-07-01T02:44:30.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>Aggregation is a decisive aspect of architecture, and as the number of modules multiply their connectivity is made all the more important.By relating architectures for aggregating channels, scales, and resolutions we identified the need for deeper aggregation, and addressed it by iterative deep aggregation and hierarchical deep aggregation. </p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p><strong>More nonlinearity, greater capacity, and larger receptive fields generally improve accuracy but can be problematic for optimization and computation.</strong> To overcome these barriers, different blocks or modules have been incorporated to balance and temper these quantities, such as bottlenecks for dimensionality reduction or residual, gated, and concatenative connections for feature and gradient propagation.Networks designed according to these schemes have 100+ and even 1000+ layers.    </p><p>Nevertheless, further exploration is needed on how to connect these layers and modules. Layered networks from LeNet  through AlexNet  to ResNet  stack layers and modules in sequence. Layerwise accuracy comparisons , transferability analysis , and representation visualization  show that deeper layers extract more semantic and more global features, <strong>but these signs do not prove that the last layer is the ultimate representation for any task</strong>. In fact, skip connections have proven effective for classification and regression and more structured tasks. Aggregation, like depth and width, is a critical dimension of architecture.    </p><p>In this work, we investigate how to aggregate layers to better fuse semantic and spatial information for recognition and localization.  <strong>We introduce two structures for deep layer aggregation (DLA): iterative deep aggregation (IDA) and hierarchical deep aggregation (HDA).</strong> </p><p><strong>IDA focuses on fusing resolutions and scales while HDA focuses on merging features from all modules and channels. IDA follows the base hierarchy to refine resolution and aggregate scale stage-by-stage. HDA assembles its own hierarchy of tree-structured connections that cross and merge stages to aggregate different levels of representation.  </strong></p><p><strong>task:</strong>  large-scale image classification, finegrained recognition, semantic segmentation, and boundary detection  </p><h3 id="Related-work"><a href="#Related-work" class="headerlink" title="Related work"></a>Related work</h3><p><strong>Our aggregation architectures are most closely related to leading approaches for fusing feature hierarchies. The key axes of fusion are semantic and spatial. Semantic fusion, or aggregating across channels and depths, improves inference of what. Spatial fusion, or aggregating across resolutions and scales, improves inference of where. Deep layer aggregation can be seen as the union of both forms of fusion.  </strong></p><h3 id="Deep-Layer-Aggregation"><a href="#Deep-Layer-Aggregation" class="headerlink" title="Deep Layer Aggregation"></a>Deep Layer Aggregation</h3><h4 id="iterative-deep-aggregation-IDA"><a href="#iterative-deep-aggregation-IDA" class="headerlink" title="iterative deep aggregation (IDA)"></a><em>iterative deep aggregation (IDA)</em></h4><p>Iterative deep aggregation follows the iterated stacking of the backbone architecture. We divide the stacked blocks of the network into stages according to feature resolution. Deeper stages are more semantic but spatially coarser. </p><ul><li><p>conventional method</p><p>(b)是FCN、FPN、U-net的方法，从粗粒度向细粒度融合。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180701/emj9hkiClD.png?imageslim" alt="mark"></p></li><li><p>IDA</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180701/B7HiEfe6I0.png?imageslim" alt="mark"></p></li></ul><h4 id="Hierarchical-Deep-Aggregation-HDA"><a href="#Hierarchical-Deep-Aggregation-HDA" class="headerlink" title="Hierarchical Deep Aggregation(HDA)"></a><em>Hierarchical Deep Aggregation(HDA)</em></h4><p>Hierarchical deep aggregation merges blocks and stages in a tree to preserve and combine feature channels. With HDA shallower and deeper layers are combined to learn richer combinations that span more of the feature hierarchy.    </p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180701/dF3mIg69E9.png?imageslim" alt="mark"></p><h4 id="aggregation-nodes"><a href="#aggregation-nodes" class="headerlink" title="aggregation nodes"></a><em>aggregation nodes</em></h4><p>特征融合：使用$1*1$的卷积</p><h4 id="blocks-and-stages"><a href="#blocks-and-stages" class="headerlink" title="blocks and stages"></a><em>blocks and stages</em></h4><p>The networks we instantiate in our experiments make use of three types of residual blocks.</p><ul><li><strong>Basic blocks</strong> combine stacked convolutions with an identity skip connection.</li><li><strong>Bottleneck blocks</strong> regularize the convolutional stack by reducing dimensionality through a 1×1 convolution. </li><li><strong>Split blocks</strong> diversify features by grouping channels into a number of separate paths (called the cardinality of the split).</li></ul><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><ul><li><p>ImageNet Classification    </p></li><li><p>Fine-grained Recognition    </p></li><li><p>Semantic Segmentation    </p></li><li><p>Boundary Detection    </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;Aggregation is a decisive aspect of architecture, and as the number of modules multiply their connectivity is made all the more important.By relating architectures for aggregating channels, scales, and resolutions we identified the need for deeper aggregation, and addressed it by iterative deep aggregation and hierarchical deep aggregation. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="CVPR" scheme="http://yoursite.com/categories/paper-notes/CVPR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/CVPR/2018/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="aggregation" scheme="http://yoursite.com/tags/aggregation/"/>
    
  </entry>
  
  <entry>
    <title>[ICASSP 2018]Advanced LSTM: A Study about Better Time Dependency Modeling in Emotion Recognition</title>
    <link href="http://yoursite.com/2018/07/01/ICASSP-2018-Advanced-LSTM-A-Study-about-Better-Time-Dependency-Modeling-in-Emotion-Recognition/"/>
    <id>http://yoursite.com/2018/07/01/ICASSP-2018-Advanced-LSTM-A-Study-about-Better-Time-Dependency-Modeling-in-Emotion-Recognition/</id>
    <published>2018-07-01T02:19:17.000Z</published>
    <updated>2019-04-28T13:33:08.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>We proposed a new type of LSTM, A-LSTM, in this paper. This was a early study of A-LSTM. We applied it in the weighted pooling RNN for emotion recognition.  </p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>LSTM每一时刻的状态依赖于上一个时刻，这种假设会约束RNN的建模容量。这篇论文，提出了一种LSTM的变体，advanced LSTM，即A-LSTM，每一时刻依赖于多个不同的时刻。</p><p>改进的方法就是利用了<u><strong>attention based weighted pooling layer</strong></u></p><h3 id="Proposed-approach"><a href="#Proposed-approach" class="headerlink" title="Proposed approach"></a>Proposed approach</h3><h4 id="attention-based-weighted-pooling-RNN"><a href="#attention-based-weighted-pooling-RNN" class="headerlink" title="attention based weighted pooling RNN"></a><em>attention based weighted pooling RNN</em></h4><p>依赖注意力机制学习每一个时间步的一个权重，$h_T$是过去的时间步，可以是一个固定的步长，例如3,4,5。</p><p><img src="https://s2.ax1x.com/2019/04/28/El9Scq.png" alt="El9Scq.png"></p><h4 id="multi-task-learning"><a href="#multi-task-learning" class="headerlink" title="multi-task learning"></a><em>multi-task learning</em></h4><p>文中的任务是基于语音的动作识别，并且是多任务的学习，包括动作、说话者以及说话者性别的识别</p><p><a href="https://imgchr.com/i/El9PBT" target="_blank" rel="noopener"><img src="https://s2.ax1x.com/2019/04/28/El9PBT.png" alt="El9PBT.png"></a></p><h4 id="advanced-LSTM"><a href="#advanced-LSTM" class="headerlink" title="advanced LSTM"></a><em>advanced LSTM</em></h4><ul><li>conventional LSTM</li></ul><p><img src="https://s2.ax1x.com/2019/04/28/El9CuV.png" alt="El9CuV.png"></p><ul><li><p>advanced LSTM</p><p>注意：<u>(9)和(10)中的W是共享的</u></p></li></ul><p><img src="https://s2.ax1x.com/2019/04/28/El9pj0.png" alt="El9pj0.png"></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;We proposed a new type of LSTM, A-LSTM, in this paper. This was a early study of A-LSTM. We applied it in the weighted pooling RNN for emotion recognition.  &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="ICASSP" scheme="http://yoursite.com/categories/paper-notes/ICASSP/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/ICASSP/2018/"/>
    
    
      <category term="emotion recognition" scheme="http://yoursite.com/tags/emotion-recognition/"/>
    
      <category term="multi-task learning" scheme="http://yoursite.com/tags/multi-task-learning/"/>
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
  </entry>
  
  <entry>
    <title>[CVPR 2018]HSA-RNN: Hierarchical Structure-Adaptive RNN for Video Summarization</title>
    <link href="http://yoursite.com/2018/06/21/CVPR-2018-HSA-RNN-Hierarchical-Structure-Adaptive-RNN-for-Video-Summarization/"/>
    <id>http://yoursite.com/2018/06/21/CVPR-2018-HSA-RNN-Hierarchical-Structure-Adaptive-RNN-for-Video-Summarization/</id>
    <published>2018-06-21T08:30:38.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>In this paper, we propose a Hierarchical Structure Adaptive RNN (HSA-RNN) for the video summarization task, which can adaptively exploit the video structure and generate the summary simultaneously. </p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>类似文本序列，视频序列也是一种层次化的结构，由帧（frame）组成镜头（shot），镜头组成视频（video）。</p><p>视频摘要（video summarization）可以分为三个类别，镜头（shots），帧（shots）以及物体（objects）。论文中讨论的是镜头级别的视频摘要，从视频中挑选出关键的shots，因为它更好地保留了视频内容的动态信息和时空一致性。</p><p>视频摘要可以分为两个阶段：1）镜头划分（shot segmentation）；2）挑选关键镜头（key shot selection）。</p><p>通常更多的方法关注的key shot selection，而shot segmentation一般根据帧突然变化、运动幅度变化，或者固定长度划分，但是这样的方法忽略了视频的结构。</p><p>针对先前shot segmentation方法的缺点，文中提出了一种层次化自适应的网络HSA-RNN，可以联合探索视频的结构和视频摘要。<strong><u>更加关注shot segmentation</u></strong></p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p>HSA-RNN包含两层：第一层探索视频的结构，划分shot；第二层根据第一层划分的shot生成视频摘要，即挑选出key shot。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/KiEk15cJLI.png?imageslim" alt="mark"></p><h4 id="layer-1"><a href="#layer-1" class="headerlink" title="layer 1"></a><em>layer 1</em></h4><p>输入所有帧的特征序列$(f_1,f_2,…,f_n)$，输出所有镜头的特征序列$(s_1,s_2,…,s_m)$。n和m分别是帧和镜头的数量。</p><p>一种简单的方法就是利用LSTM应用于整个视频，每一步通过一个输出阈值判断shot的边界，如图(a)。但是这种方法有两个缺点：</p><p>1）难以应用双向LSTM到这种结构，在检测shot边界时前后的信息都是很重要的。</p><p>2）阈值难以设定。</p><p>文中使用了<strong><u>滑动的双向LSTM</u></strong>解决这个问题，也是论文的亮点，如图(b)。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/G207FmiI02.png?imageslim" alt="mark"></p><p>如图(b)，它就像一个固定长度的一维滤波器在帧序列上滑动。双向LSTM的长度$k$凭经验选择（文中k=240），LSTM每次滑动的长度等于检测出的镜头（detected shot）的长度。</p><p>步骤：</p><ul><li>某一时刻双向LSTM处理的帧序列$(f_1,f_2,..,f_k)$，经过隐藏层后有<br>$$<br>h_t^{1,f}=LSTM(f_t,h_{t-1}^{1,f})\<br>h_t^{1,b}=LSTM(f_t,h_{t+1}^{1,b})<br>$$<br>$h_t^{1,f}$和$h_t^{1,b}$是双向LSTM的隐藏层状态。将这两个状态拼接用来计算每一帧的作为shot的边界的置信度$c_t$。<br>$$<br>c_t=softmax(Relu(W_c[h_t^{1,f},h_t^{1,b}]+b_c))<br>$$<br>其中，$c_t$是一个二维向量，包含了是帧作为shot的边界的置信度与否。<br>$$<br>t^<em>=max{c_1(1),c_2(1),…,c_k(1)}<br>$$<br>将$h_{t^</em>}^{1,f}$作为shot的边界。滑动的LSTM下一次处理的帧序列为$(f_{t^<em>+1},f_{t+2},…,f_{t^</em>+k})$。</li></ul><h4 id="layer-2"><a href="#layer-2" class="headerlink" title="layer 2"></a><em>layer 2</em></h4><p>同样利用双向LSTM处理layer 1得到的shot序列$(s_1,s_2,…,s_m)​$。<br>$$<br>h_t^{2,f}=LSTM(s_t,h_{t-1}^{2,f})\<br>h_t^{2,b}=LSTM(s_t,h_{t+1}^{2,b})<br>$$<br><u>将隐藏层状态和$s_t$拼接</u>，计算每一shot作为key shot生成视频摘要的置信度。<br>$$<br>p_t=softmax(Relu(W_p[h_t^{2,f};h_t^{2,b};s_t]+b_p))<br>$$<br>损失函数：</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/8A9dJ8KGik.png?imageslim" alt="mark"></p><p>T为视频的数量，$n^{(i)}$是视频的帧数。考虑到生成的shot的长度和人为设定的有不同的间隔差异，这里的预测是frame-level的预测$\hat{p}_t^{(i)}$，通过得到的shot的预测结果分配给shot所包含的frame。$L(\cdot)$为交叉熵。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><strong>datasets</strong></p><ul><li>SumMe</li><li>TVSum</li><li>CoSum</li><li>VTW</li></ul><p><strong>results</strong></p><ul><li><p>boundary detection</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/LIj24fB2Fj.png?imageslim" alt="mark"></p></li><li><p>video summatization </p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/f0B3bh76AE.png?imageslim" alt="mark"></p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/mbAaFfH1LE.png?imageslim" alt="mark"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;In this paper, we propose a Hierarchical Structure Adaptive RNN (HSA-RNN) for the video summarization task, which can adaptively exploit the video structure and generate the summary simultaneously. &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="CVPR" scheme="http://yoursite.com/categories/paper-notes/CVPR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/CVPR/2018/"/>
    
    
      <category term="video summarization" scheme="http://yoursite.com/tags/video-summarization/"/>
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
  </entry>
  
  <entry>
    <title>[CVPR 2018]Dual Skipping Networks</title>
    <link href="http://yoursite.com/2018/06/20/CVPR-2018-Dual-Skipping-Networks/"/>
    <id>http://yoursite.com/2018/06/20/CVPR-2018-Dual-Skipping-Networks/</id>
    <published>2018-06-20T15:30:58.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>Inspired by the recent study on the hemispheric specialization and coarse-to-fine perception, we proposed a novel left-right asymmetric layer skippable network for coarse-to-fine object categorization.    </p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>在人类的大脑上，尽管关于视觉分析的具体位置和方法还有很多的争论，但是有相当多的证据证明，视觉分析主要发生在一个默认的粗到细的序列上（coarse-to-fine sequence），如图（1）。例如，快速地给某人看图片（1）时，只能接收到非常粗的视觉刺激，只能看到砂子和雨伞，通常是低空间频率的（Low spatial frequencies）。如果给定一个较长的观察时间，可以接收到一些细的视觉感知，通常时高空间频率的（high spatial frequencies）。</p><p>最近生物研究表明，大脑的两个半球功能在处理空间频率信息（spatial frequency information）时并不是完全一样的，左半球（LH）主要处理高空间频率信息，而右半球（RH）主要处理低空间频率信息，如图（2-A，2-B）。</p><p>受到大脑结构的启发，文中提出了dual skipping network（DSN），网络有左右两个分支，这两个分支不对称，并且层可以跳跃。模型可以用一个简单的框架实现粗到细的物体分类。</p><p>尽管空间频率不能等同于识别的粒度，DSN的工作原理和大脑半球形似，取决于监督信息的粒度。</p><p><strong>Contributions</strong>     </p><ul><li>受到最近神经科学研究的启发，提出左右分支网络结构解决粗到细的物体分类任务。</li><li>提出层跳跃机制（layer-skipping mechanism），在测试阶段可以跳跃一些层。</li><li>采用自顶向下的反馈机制，通过高层的global语义信息来指导细粒度分类。</li><li>构建一个新的数据集small-big MNIST，每个MNIST的global数字由local数字组成。</li></ul><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/2bmldBD0cf.png?imageslim" alt="mark"></p><h3 id="Model"><a href="#Model" class="headerlink" title="Model"></a>Model</h3><p>模型由左右两个分支组成，每个分支由shared convolutional layer，skip-dense blocks，transition layers，pooling layer和classification layer组成。</p><p>左分支负责细粒度分类，右分支负责粗粒度分类，左右分支的结构是一样的，但是右分支有“Guide”箭头指导左分支。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/bbdhbLfaj0.png?imageslim" alt="mark"></p><h4 id="shared-convolutional-layer"><a href="#shared-convolutional-layer" class="headerlink" title="shared convolutional layer"></a><em>shared convolutional layer</em></h4><p>两个分支的共享卷积层。</p><h4 id="Left-Right-subnet"><a href="#Left-Right-subnet" class="headerlink" title="Left/Right subnet"></a><em>Left/Right subnet</em></h4><p>两个分支都是由skip-dense blocks和transition交替堆叠而成。</p><p>skip-dense blocks结构如上图最右所示，可以看作是一个级别的视觉感知抽象，其中gating network学习当前的块信息是否流向dense layer。</p><p>transition由$1\times1$的卷积和池化操作组成，控制下一个skip-dense block的特征图数量和空间大小。</p><h4 id="skip-dense-block"><a href="#skip-dense-block" class="headerlink" title="skip-dense block"></a><em>skip-dense block</em></h4><p>skip-dense block由一系列的gating network和dense layer堆叠而成，其中每个gating network和dense layer可以看作resNet中的残差连接。</p><p><strong><u>dense layer</u></strong>     dense layer是由denseNet或者resNet组成。denseNet和resNet主要不同在于，残差连接的方式不一样，denseNet是通道拼接，resNet是按元素相加。</p><p><strong><u>gating network</u></strong>     gating network负责路径的选择。对于N维的特征输入，经过一个$N\times1$的全连接层得到一个标量输出，在通过一个阈值函数（threshold function）。</p><p><strong><u>threshold function of gating network</u></strong>      给定一个激活值或者输入，阈值函数需要判断是否要跳过接下来的dense layer。直观上，阈值函数是一个二分类，文中采用了hard sigmoid function。<u>文中的实验中显示hard sigmoid要比soft sigmoid效果要好，soft sigmoid就是常用的sigmoid函数。</u><br>$$<br>hard \ sigm(x)=max(0,min(kx+\frac{1}{2},1))<br>$$<br>斜率参数$k$ 是一个关键参数，决定dense layer的输出缩放比例，$k$初始化为1，每个epoch按照一个固定值增加。</p><p>hard sigmoid是logistic sigmoid激活函数的分段线性近似，它更易计算，使得学习计算的速度更快，尽管首次派生值为零可能导致静默神经元过慢的学习率。</p><p>阈值函数会和卷积输出的每个单元相乘，影响层的分类重要性。</p><h4 id="Guide"><a href="#Guide" class="headerlink" title="Guide"></a><em>Guide</em></h4><p>较快的coarse/global分支可以以自顶向下的方式，通过global上下文信息引导较慢的fine/local分支。模型中，选择粗分支的最后一个skip-dense block的输出特征，去引导细分支中最后一个transition layer。</p><p>粗分支的最后一个skip-dense block的输出特征通过双线性采样和细分支的最后一个transition layer的输入特征拼接，输入给细分支的最后一个transition layer。</p><h3 id="Experiments"><a href="#Experiments" class="headerlink" title="Experiments"></a>Experiments</h3><p><strong>Datasets</strong></p><ul><li>sb-MNIST（不采用guide）</li><li>CIFAR-100</li><li>CUB-200-2011</li><li>stanford cars</li></ul><p><strong>baselines</strong></p><ul><li>Feedback Net</li><li>DenseNet</li><li>ResNet</li></ul><p><strong>results</strong></p><ul><li><p>sb-MNIST result</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/F26CBdC7bE.png?imageslim" alt="mark"></p></li><li><p>CIFAR-100</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/FFkLk0gF5J.png?imageslim" alt="mark"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;Inspired by the recent study on the hemispheric specialization and coarse-to-fine perception, we proposed a novel left-right asymmetric layer skippable network for coarse-to-fine object categorization.    &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="CVPR" scheme="http://yoursite.com/categories/paper-notes/CVPR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/CVPR/2018/"/>
    
    
      <category term="object classification" scheme="http://yoursite.com/tags/object-classification/"/>
    
      <category term="layyer skipping" scheme="http://yoursite.com/tags/layyer-skipping/"/>
    
      <category term="coarse-to-fine" scheme="http://yoursite.com/tags/coarse-to-fine/"/>
    
  </entry>
  
  <entry>
    <title>[CVPR 2018]A Neural Multi-sequence Alignment TeCHnique (NeuMATCH)</title>
    <link href="http://yoursite.com/2018/06/19/CVPR-2018-A-Neural-Multi-sequence-Alignment-TeCHnique-NeuMATCH/"/>
    <id>http://yoursite.com/2018/06/19/CVPR-2018-A-Neural-Multi-sequence-Alignment-TeCHnique-NeuMATCH/</id>
    <published>2018-06-19T15:42:10.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种端到端训练的模型，称为NeuMATCH，用于处理多个异源（即对齐的两个目标的来源不同，例如视频和文本的对齐）序列对齐的问题。</p></blockquote><a id="more"></a><h3 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h3><p>序列对齐主要包含三种类型：一对一，一对多，非单调。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/aeCh1DLEf9.png?imageslim" alt="mark"></p><p>传统的序列对齐方法包括两个阶段：1）学习两个句子的相似性度量；2）找到最优的对齐路径。例如DTW，CTW。</p><p>基于DTW的方法属于马尔科夫假设，只考虑了局部的上下文信息，然而有利于对齐的上下文信息可能分散在整个序列，例如一些叙述性的电影。</p><p>文中采用的是视频和文本对齐的数据，尤其是包含一些叙述性的内容，例如电影。选择叙述性的内容原因在于，由于事件之间存在大量的因果和时间相互作用，是计算理解中最具挑战性的。</p><p><strong>Contributions</strong>  1）论文提出了一种端到端的循环框架，用于异源端个序列对齐问题。不像之前的方法，我们在决策时考虑了更多的上下文信息；2）标注了一个新的<a href="https://github.com/pelindogan/NeuMATCH" target="_blank" rel="noopener">数据集</a> ，包含了电影摘要视频。</p><h3 id="Approach"><a href="#Approach" class="headerlink" title="Approach"></a>Approach</h3><p><strong>任务</strong>：给定两个序列，视频序列和文本序列。视频序列包含了一系列连续的镜头（video clips），记为$\mathcal{V}=_{i=1,2…,N}$ 。文本序列包含了一系列连续的句子，记为${\mathcal{S}}=_{i,2,…,M}$ 。任务是找到一个函数$\pi$将视频段的下标映射到相应的句子：$\langle V_i,S_i \rangle$ 。</p><p><strong><u>由于需要学习相似性度量的上下文信息分散在整个序列上，所以在决策时需要考虑过去和未来的信息。</u></strong> </p><h3 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h3><p>整个模型由四个LSTM组成，包含了输入视频序列（Video Stack），输入文本序列（Text Stack），先前对齐动作（Action Stack），先前对齐匹配（Match Stack）。</p><p><strong><u>这里的stack不是堆叠的意思，而是数据结构中的stack，先入先出。这也是本文的一个亮点。</u></strong></p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/mKc05J5ag4.png?imageslim" alt="mark">  </p><h4 id="Language-and-Visual-Encoders"><a href="#Language-and-Visual-Encoders" class="headerlink" title="Language and Visual Encoders"></a><em>Language and Visual Encoders</em></h4><p>先对每个镜头（video clips）和每个句子编码，然后在通过可选择的预训练，将编码后对的镜头和句子嵌入到同一个空间。通过预训练可以得到很好的初始化。</p><p><strong>Video Encoder</strong>  利用VGG-16训练镜头的每一帧，将第一层全连接层的特征提取出来，为4096维。对所有帧的特征进行mean pooling，作为每个镜头的特征。镜头特征在经过三层全连接层，最后的到video的编码信息$v_i$，表示第i个镜头（clip）的编码信息。</p><p><strong>Sentence Encoder</strong>   对每个词使用GloVe嵌入，在经过两层LSTM后，得到最后一层隐藏层的特征$h_t$，在经过三层全连接层得到每个句子的编码信息$s_i$。</p><p><strong>Encoding Alignment and Pre-training</strong>    文中采用了一种可选择的预训练，可以产生一个好的初始化。这里的初始化使用了<a href="https://arxiv.org/abs/1511.06361" target="_blank" rel="noopener">order-embedding</a> ，对于每一对ground-truth  $(V_i,S_i)$，采用一种非对称的相似度度量：<br>$$<br>F(v_i,s_i)=-||max(0,v_i,s_i)||^2<br>$$<br>相似度函数最大值为0。为每对ground-truth随机采样一些对比的clip $V’$和句子$S’$ 。最小化损失函数使得groud-truth的相似度为0，而对比样本的相似度低于一个间隔$\alpha$ 。损失函数为</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180621/ckKaHfJHaA.png?imageslim" alt="mark"> </p><h4 id="The-NeuMATCH-Alignment-Network"><a href="#The-NeuMATCH-Alignment-Network" class="headerlink" title="The NeuMATCH Alignment Network"></a><em>The NeuMATCH Alignment Network</em></h4><p><strong>LSTM Stacks</strong> </p><ul><li><p><em>Video Stack</em>   对于每一时刻$t$，video stack包含了还没处理的序列$V_t,V_{t+1},…,V_N$ 。LSTM的方向是从$V_N$到$V_t$，允许信息从未来的clip流到当前的clip，将这个LSTM作为video stack，其隐藏层特征为$h_t^V$。</p></li><li><p><em>sentence stack</em>  处理方法和video stack一样， 对于每一时刻$t$，sentence stack包含了还没处理的序列$S_t,S_{t+1},…,S_N$，其隐藏层特征为$h_t^S$。</p></li><li><p><em>action stack</em>   action stack负责储存过去所有的对齐动作，动作极为$A_{t-1},…,A_1$，每个动作为one-hot编码。动作的信息流不同于video stack和sentence stack，信息是从第一个动作流向最后一个动作，其隐藏层状态记为$h_{t-1}^A$。</p><p>文中定义了五种基本的对齐动作，它们一起处理两个序列对齐的问题，包含了不匹配（例如，一个clip没有句子和它匹配，记为null）以及一对多匹配（一个句子和多个clip匹配）。</p><p>五个对齐动作分别为：Pop Clip (PC),Pop Sentence (PS), Match (M), Match-Retain Clip (MRC), and Match-Retain Sentence (MRS)。下面是一个操作例子。</p><p><strong><u>需要注意的是，并不是这五个动作都需要一起使用，具体问题具体分析，例如一对一的对齐，每个句子和某个clip匹配，允许某个clip和没有句子匹配，则只需要Pop Clip以及Match；一对多的对齐，允许每个句子和多个clip匹配，允许某个clip和没有句子匹配，每个句子至少和一个clip匹配，则只需要Pop Clip, Pop Sentence, and Match- Retain Sentence。</u></strong></p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/5bF8KCldig.png?imageslim" alt="mark"></p></li></ul><ul><li><em>matched stack</em>   match stack包含了先前已经匹配的clip和sentence，其中最后匹配的置于stack的顶部。本文考虑一个句子可以匹配多个clip。因为匹配的clip在内容上是相似的，所以一个句子匹配多个clip的，取它们的均值，即$v_i=\sum_j^Kv_j/K$。则LSTM的输入为sentence和clip的拼接，$r_i=[s_i,v_I]$ 。最后一个隐藏层状态为$h_{t-1}^M$。</li></ul><p><strong>Alignment Action Prediction</strong></p><p>每一时刻$t$，四个stack的状态为$\Psi_t=(V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})$ 。则第$t$时刻$A_t$的条件概率为<br>$$<br>P(A_t|\Psi_t)=P(A_t|V_{t^+},S_{t^+},A_{(t-1)^-},R_{1^+})<br>$$<br>上面的计算方式为$\psi_t$经过两层全连接层然后在softmax。</p><p>根据链式法则，<br>$$<br>P(A_1,…,A_N|\mathcal{V},\mathcal{S})=\prod_{t=1}^NP(A_t|A_{(t-1)^-},\Psi_t)<br>$$<br>将交叉熵作为损失函数。</p><h3 id="Experimental-Evaluation"><a href="#Experimental-Evaluation" class="headerlink" title="Experimental Evaluation"></a>Experimental Evaluation</h3><p><strong>dataset</strong></p><p>HM-1为在序列中随机插入一些不同电影的片段，但是具有相似的情节，HM-2随机删除一些句子，YMS是论文提出的new dataset，数来源于youtube。</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/I4giJf559F.png?imageslim" alt="mark"></p><p><strong>baselines</strong></p><ul><li>Minimum Distance (MD),</li><li>Dynamic Time Warping (DTW)</li><li>Canonical Time Warping (CTW)</li></ul><p><strong>results</strong></p><ul><li><p>一对一</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/ECgI7i0E11.png?imageslim" alt="mark"></p></li><li><p>一对多</p><p><img src="http://pakzslacd.bkt.clouddn.com/blog/180620/7e4FiEmA4i.png?imageslim" alt="mark"></p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种端到端训练的模型，称为NeuMATCH，用于处理多个异源（即对齐的两个目标的来源不同，例如视频和文本的对齐）序列对齐的问题。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="CVPR" scheme="http://yoursite.com/categories/paper-notes/CVPR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/CVPR/2018/"/>
    
    
      <category term="sequence alignment" scheme="http://yoursite.com/tags/sequence-alignment/"/>
    
  </entry>
  
  <entry>
    <title>[NIPS 2017]Learning Hierarchical Information Flow with Recurrent Neural Modules</title>
    <link href="http://yoursite.com/2018/06/10/NIPS-2017-Learning-Hierarchical-Information-Flow-with-Recurrent-Neural-Modules/"/>
    <id>http://yoursite.com/2018/06/10/NIPS-2017-Learning-Hierarchical-Information-Flow-with-Recurrent-Neural-Modules/</id>
    <published>2018-06-09T17:35:44.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文受到新脑皮质（neocortical）通信方式的启发，提出了ThalNet。新脑皮质通信方式有两种，一种是直连，另一种是通过丘脑（thalamus）。论文受到第二种通信方式的启发，构建多个循环神经模块，将所有模块的特征发送到一个路由中心，使得模块在多个时间步能够共享特征。模型展示了模块对输入数据的链式处理，学习了层次化的路由信息，并且模型包含了前馈神经网络、跳跃连接、反馈连接等结构。</p></blockquote><a id="more"></a><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-1.jpg?raw=true" alt=""></p><p>论文构建的模型包括四个模块，模块$f^1$接受input， $f^2$，$f^3$为侧模块，$f^4$负责输出，每个模块$f$可以是全连接层、GRU、LSTM等。对于每一时刻t，每个模块将其特征$\phi_t$发给路由中心 $\Phi$，其中每个模块特征由上下文$c_t$和一个可以选择的输入 $x_t$决定。对于下一时刻的上下文 $c_{t+1}$，每个模块通过阅读机制（read mechanism）选择性地读取路由中心$\Phi$的特征。对于每个模块的特征计算，上下文计算，路由中心，模型输出式子见下图。</p><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-2.jpg?raw=true" alt=""></p><p>阅读机制包括两种：静态阅读，动态阅读。静态阅读只取决于 $\Phi$，即$r^i(\Phi)$，而动态阅读不仅取决于 $\Phi$，还取决于当前模块的特征，即 $r^i(\Phi,\phi^i)$。</p><p>文中给出了四种阅读方法，其中weight normalization、Fast softmax比较稳定，效果比较好。 </p><p><img src="https://github.com/yesic/pic/blob/master/hif/hif-3.jpg?raw=true" alt=""><img src="https://github.com/yesic/pic/blob/master/hif/hif-4.jpg?raw=true" alt=""></p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>论文在三个任务上做了实验， Sequential Permuted MNIST、Sequential CIFAR-10、Text8 Language Modeling。其中前两个是图像的延迟预测，即将图像的像素每一行看作一个时刻，在输入最后一个时刻时预测出图像的类别，第三个任务是输入每个character，预测下一个character。 </p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文受到新脑皮质（neocortical）通信方式的启发，提出了ThalNet。新脑皮质通信方式有两种，一种是直连，另一种是通过丘脑（thalamus）。论文受到第二种通信方式的启发，构建多个循环神经模块，将所有模块的特征发送到一个路由中心，使得模块在多个时间步能够共享特征。模型展示了模块对输入数据的链式处理，学习了层次化的路由信息，并且模型包含了前馈神经网络、跳跃连接、反馈连接等结构。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="NIPS" scheme="http://yoursite.com/categories/paper-notes/NIPS/"/>
    
      <category term="2017" scheme="http://yoursite.com/categories/paper-notes/NIPS/2017/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
  </entry>
  
  <entry>
    <title>[ICLR 2018]Hierarchical Representations for Efficient Architecture Search</title>
    <link href="http://yoursite.com/2018/06/10/ICLR-2018-Hierarchical-Representations-for-Efficient-Architecture-Search/"/>
    <id>http://yoursite.com/2018/06/10/ICLR-2018-Hierarchical-Representations-for-Efficient-Architecture-Search/</id>
    <published>2018-06-09T17:24:00.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文结合了一种新型层次化遗传表示体系（hierarchical genetic representation scheme），可以模仿人类专家常用的模块化设计模式，支持复杂的拓扑结构。分别在CIFAR-10和ImageNet上获得了top-1错误率为3.6%和20.3%。 </p></blockquote><a id="more"></a><p>在这项工作中，作者通过强加一个层次化的网络来限制搜索空间结构，允许每层是一些灵活的网络拓扑结构（有向无环图）。底层是一些基本操作，例如卷积，池化等，更高层的计算图，或者modif，是由低层modif作为它们的构建块来组成，最高层的modif通过堆叠形成最终的网络结构。如下图所示。</p><p><img src="https://github.com/yesic/pic/blob/master/hgrs/hgrs-1.jpg?raw=true" alt=""></p><p>作者通过进化搜索或者随机搜索来发现层次化结构。基于遗传算法，作者定义了一些变异（mutation），每次选择某一层的某个motif的某条边发生变异，这可以使得modif的结构发生改变（删除，添加，修改），将验证集的准确率作为fitness。</p><p>在实验中，提出的搜索框架只学习cell的结构，而不是整个模型。原因是这样能够快速计算fitness，然后可以将对应的genotype转移到大的模型，也就是用更少的cell计算fitness，更多的cell评估模型。下图是使用框架搜索优化过的cell构建的图像分类模型。</p><p><img src="https://github.com/yesic/pic/blob/master/hgrs/hgrs-2.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文结合了一种新型层次化遗传表示体系（hierarchical genetic representation scheme），可以模仿人类专家常用的模块化设计模式，支持复杂的拓扑结构。分别在CIFAR-10和ImageNet上获得了top-1错误率为3.6%和20.3%。 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="ICLR" scheme="http://yoursite.com/categories/paper-notes/ICLR/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/ICLR/2018/"/>
    
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
  </entry>
  
  <entry>
    <title>[IJCAI 2018]A Hierarchical End-to-End Model for Jointly Improving Text Summarization and Sentiment Classification</title>
    <link href="http://yoursite.com/2018/06/10/IJCAI-2018-A-Hierarchical-End-to-End-Model-for-Jointly-Improving-Text-Summarization-and-Sentiment-Classification/"/>
    <id>http://yoursite.com/2018/06/10/IJCAI-2018-A-Hierarchical-End-to-End-Model-for-Jointly-Improving-Text-Summarization-and-Sentiment-Classification/</id>
    <published>2018-06-09T16:44:10.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种层次化端到端模型，整合了文本摘要生成和情感分类任务 。</p></blockquote><a id="more"></a><blockquote><p>模型借鉴来源：<a href="https://yesic.github.io/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/" target="_blank" rel="noopener">Hierarchical Attention Networks for Document Classification</a></p></blockquote><hr><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>给定样本$(x^i,y^i,l^i)$，包含了文本，摘要，情感标签，同时进行摘要生成和情感分类。其中$L_i$为文本的单词数， $M_i$为摘要的单词数。</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-1.jpg?raw=true" alt=""><br><br></div><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>模型包括三部分：text encoder，summary decoder，sentiment classifier</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-2.jpg?raw=true" alt=""><br><br></div><h5 id="text-encoder"><a href="#text-encoder" class="headerlink" title="text encoder"></a>text encoder</h5><ol><li>使用双向LSTM作为encoder，产生上下文信息$h={h_1,h_2,…,h_L}$</li></ol><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-3.jpg?raw=true" alt=""></div><h5 id="summary-decoder"><a href="#summary-decoder" class="headerlink" title="summary decoder"></a>summary decoder</h5><p>Summary decoder包括三部分：单向LSTM，multi-view attention，word generator</p><ol><li><p>利用单向LSTM产生decoder的隐藏层输出$s_t$</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-4.jpg?raw=true" alt=""></div></li><li><p>利用multi-view attention生成摘要向量$\mathbf{v}^{(c)}$和情感向量$\mathbf{v}^{(t)}$，multi-view attention其实就是两个独立的global attention。摘要向量$\mathbf{v}^{(c)}$生成如下：</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-5.jpg?raw=true" alt=""></div></li><li><p>根据摘要向量 $\mathbf{v}^{(c)}$，利用生成摘要中的每个词。 </p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-6.jpg?raw=true" alt=""></div></li></ol><h5 id="sentiment-classifier"><a href="#sentiment-classifier" class="headerlink" title="sentiment classifier"></a>sentiment classifier</h5><ol><li><p>将所有时间步的情感向量 $\mathbf{v}^{(t)}$收集起来</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-7.jpg?raw=true" alt=""></div></li><li><p>为了获得原文本的上下文信息，使用highway机制，将上下文信息h作为分类器输入的一部分，r为情感分类的输入向量</p><div align="center"><br><br><img src="https://github.com/yesic/pic/blob/master/man/man-8.jpg?raw=true" alt=""></div></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在亚马逊在线评论数据集上做了实验，在和其他模型的对比上均获得了最好的结果。</p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种层次化端到端模型，整合了文本摘要生成和情感分类任务 。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="IJCAI" scheme="http://yoursite.com/categories/paper-notes/IJCAI/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/IJCAI/2018/"/>
    
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="text summarization" scheme="http://yoursite.com/tags/text-summarization/"/>
    
      <category term="sentiment classification" scheme="http://yoursite.com/tags/sentiment-classification/"/>
    
  </entry>
  
  <entry>
    <title>[NAACL 2016]Hierarchical Attention Networks for Document Classification</title>
    <link href="http://yoursite.com/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/"/>
    <id>http://yoursite.com/2018/06/10/NAACL-2016-Hierarchical-Attention-Networks-for-Document-Classification/</id>
    <published>2018-06-09T16:19:35.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种层次化的注意力网络用于文本分类，模型有两个特点：1）它有一个层次化的结构，反映了文本的层次化结构（由词组成句；由句组成文本）；2）它有两个level的注意力机制，分别是word-level和sentence-level。 </p></blockquote><a id="more"></a><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>模型由四部分组成：word encoder，word attention，sentence encoder，sentence attention</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-1.jpg?raw=true" alt=""></p><h5 id="word-encoder"><a href="#word-encoder" class="headerlink" title="word encoder"></a>word encoder</h5><ol><li><p>对每个词进行word embedding，在通过双向GRU提取隐藏层状态，将两个方向的隐藏层状态拼接， $w_{it}$为文本第i个句子的第t个词 。</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-2.jpg?raw=true" alt=""></p></li></ol><p><img src="https://github.com/yesic/pic/blob/master/han/han-3.jpg?raw=true" alt=""></p><h5 id="word-attention"><a href="#word-attention" class="headerlink" title="word attention"></a>word attention</h5><ol><li>对每个句子的所有词做attention，然后计算每个句子向量$s_i$：</li></ol><p><img src="https://github.com/yesic/pic/blob/master/han/han-4.jpg?raw=true" alt=""></p><h5 id="sentence-encoder"><a href="#sentence-encoder" class="headerlink" title="sentence encoder"></a>sentence encoder</h5><ol><li><p>和word encoder方式一样，利用双向GRU提取每个句子向量$s_i$的信息，然后将两个方向的隐藏层状态拼接</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-5.jpg?raw=true" alt=""></p><p><img src="https://github.com/yesic/pic/blob/master/han/han-6.jpg?raw=true" alt=""></p></li></ol><h5 id="sentence-attention"><a href="#sentence-attention" class="headerlink" title="sentence attention"></a>sentence attention</h5><p>和word attention方式一样，对文本的每个句子做attention，然后计算文本向量v，将v用softmax分类。</p><p><img src="https://github.com/yesic/pic/blob/master/han/han-7.jpg?raw=true" alt=""></p><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在6个大规模文本分类数据集上做了实验，和众多SVM，CNN，LSTM比较均获得了最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/han/han-8.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种层次化的注意力网络用于文本分类，模型有两个特点：1）它有一个层次化的结构，反映了文本的层次化结构（由词组成句；由句组成文本）；2）它有两个level的注意力机制，分别是word-level和sentence-level。 &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="NAACL" scheme="http://yoursite.com/categories/paper-notes/NAACL/"/>
    
      <category term="2016" scheme="http://yoursite.com/categories/paper-notes/NAACL/2016/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="document classification" scheme="http://yoursite.com/tags/document-classification/"/>
    
  </entry>
  
  <entry>
    <title>[arxiv 2018]Hierarchical Attention-Based Recurrent Highway Networks for Time Series Prediction</title>
    <link href="http://yoursite.com/2018/06/09/arxiv-2018-Hierarchical-Attention-Based-Recurrent-Highway-Networks-for-Time-Series-Prediction/"/>
    <id>http://yoursite.com/2018/06/09/arxiv-2018-Hierarchical-Attention-Based-Recurrent-Highway-Networks-for-Time-Series-Prediction/</id>
    <published>2018-06-09T14:21:15.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了一种可以端到端学习的深度学习模型，Hierarchical Attention-Based Recurrent Highway Networks(HARNN)，可以将外部序列的时空特征提取和目标序列的时空动态建模结合到一个简单的框架。</p></blockquote><a id="more"></a><blockquote><p>论文模型借鉴来源：<a href="https://yesic.github.io/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/" target="_blank" rel="noopener">A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</a></p></blockquote><hr><h4 id="模型贡献"><a href="#模型贡献" class="headerlink" title="模型贡献"></a>模型贡献</h4><ul><li>利用卷积网络学习外部输入（<strong>x</strong>）中变量之间的空间特征。接着利用RHN（recurrent highway network）在不同层构建不同的语义，建模时序动态。</li><li>提出层次化注意力机制。</li><li>在获得高准确率的同时，可以捕获时间序列中的突然振荡或者变动。</li></ul><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>时间序列预测，给定<code>1~T-1</code>时刻的目标序列以及<code>1~T-1</code>时刻的外部序列，预测T时刻的目标值。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p>encoder和decoder均使用多层RHN，在encoder使用卷积网络提取外部序列的时空特征，在decoder使用层次化注意力机制对时序依赖动态建模。</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-1.jpg?raw=true" alt=""></p><h5 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h5><ol><li><p>将外部输入$(\mathbf{x_1,x_2,…,x_{T-1}})$通过几层卷积、池化操作后输给一个全连接层，得到一个特征向量 $(\mathbf{w_1,w_2,…,w_{T-1}})$ </p></li><li><p>利用RHN对<code>encoder-1</code>中的特征向量进行时序动态建模。$\mathbf{h}_t^{[k]}$表示第k层第t时刻的隐藏层状态 </p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-2.jpg?raw=true" alt=""></p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-3.jpg?raw=true" alt=""></p></li></ol><h5 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h5><ol><li><p>多层注意力机制：在t时刻，对encoder每一层的1~T-1时刻的隐藏层状态做attention。其中$\mathbf{s_{t-1}}=\mathbf{s}_{t-1}^{[k]}$ ，t-1时刻decoder第k层隐藏层状态。</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-4.jpg?raw=true" alt=""></p></li><li><p>则第k层第t时刻的子上下文$\mathbf{d}_t^{[k]}$为</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-5.jpg?raw=true" alt=""></p><p>将t时刻所有层的子上下文拼接，得到t时刻的上下文$\mathbf{d}_t$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-6.jpg?raw=true" alt=""></p></li><li><p>更新decoder输入$\mathbf{y}_t$为$\mathbf{\tilde{y}_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-7.jpg?raw=true" alt=""></p></li><li><p>解码器RHN第k层隐藏层状态$\mathbf{s}_{t}^{[k]}$：</p><p> <img src="https://github.com/yesic/pic/blob/master/harnn/harnn-8.jpg?raw=true" alt=""></p></li><li><p>第T时刻预测值$\mathbf{\hat{y}_T}$：</p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-9.jpg?raw=true" alt=""></p></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在三个数据集上和ARIMA，LSTM，GRU，DA-RNN模型比较，均获得最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/harnn/harnn-10.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了一种可以端到端学习的深度学习模型，Hierarchical Attention-Based Recurrent Highway Networks(HARNN)，可以将外部序列的时空特征提取和目标序列的时空动态建模结合到一个简单的框架。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="arxiv" scheme="http://yoursite.com/categories/paper-notes/arxiv/"/>
    
      <category term="2018" scheme="http://yoursite.com/categories/paper-notes/arxiv/2018/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="time series prediction" scheme="http://yoursite.com/tags/time-series-prediction/"/>
    
  </entry>
  
  <entry>
    <title>[IJCAI 2017]A Dual-Stage Attention-Based Recurrent Neural Network for Time Series Prediction</title>
    <link href="http://yoursite.com/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/"/>
    <id>http://yoursite.com/2018/06/09/IJCAI-2017-A-Dual-Stage-Attention-Based-Recurrent-Neural-Network-for-Time-Series-Prediction/</id>
    <published>2018-06-09T10:46:59.000Z</published>
    <updated>2019-04-28T13:47:18.000Z</updated>
    
    <content type="html"><![CDATA[<blockquote class="blockquote-center"><p>论文提出了基于注意力机制的两阶段循环神经网络（DA-RNN），在第一阶段（encoder），引入input attention mechanism对每一时刻的外部输入自适应性地提取相关性；在第二阶段（decoder），引入temporal attention mechanism捕获encoder的长期时序依赖信息。</p></blockquote><a id="more"></a><h4 id="任务"><a href="#任务" class="headerlink" title="任务"></a>任务</h4><p>时间序列预测，给定<code>1~T-1</code>时刻的目标序列以及<code>1~T-1</code>时刻的外部序列，预测T时刻的目标值。</p><h4 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h4><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-1.jpg?raw=true" alt="DA-RNN"></p><h5 id="encoder"><a href="#encoder" class="headerlink" title="encoder"></a>encoder</h5><ol><li><p>使用LSTM作为encoder和decoder，其中状态更新如下 ：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-2.jpg?raw=true" alt=""></p></li><li><p>引入input attention mechanism：对每一时刻$\mathbf{X_t}$的n维变量使用attention</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-3.jpg?raw=true" alt=""> </p></li><li><p>使用$\alpha_t^k$对$\mathbf{X_t}$加权求和，更新$\mathbf{X_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-4.jpg?raw=true" alt=""></p></li><li><p>使用$\tilde{X_t}$更新<code>1</code>中的状态方程：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-5.jpg?raw=true" alt=""></p></li></ol><h5 id="decoder"><a href="#decoder" class="headerlink" title="decoder"></a>decoder</h5><ol><li><p>引入temporal attention mechanism：在decoder的第t时刻，对encoder所有隐藏层状态做attention </p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-6.jpg?raw=true" alt=""></p></li><li><p>计算t时刻的上下文向量$\mathbf{c_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-8.jpg?raw=true" alt=""></p></li><li><p>利用$\mathbf{c_t}$更新目标序列的输入值$\mathbf{y_{t-1}}$为$\mathbf{\tilde{y}_{t-1}}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-7.jpg?raw=true" alt=""></p></li><li><p>更新第t时刻decoder的隐藏层状态$\mathbf{d_t}$：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-9.jpg?raw=true" alt=""></p><p>其中$f_2$的更新状态方程如<code>encoder-1</code>：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-10.jpg?raw=true" alt=""></p></li></ol><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-11.jpg?raw=true" alt=""></p><ol><li><p>则第t时刻的预测值$\mathbf{\hat{T}_t}$为：</p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-12.jpg?raw=true" alt=""></p></li></ol><h4 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h4><p>作者在两个数据集上和ARIMA，NARX RNN，Encoder-Decoder，Attention RNN模型进行了比较，均获得了最好的结果。 </p><p><img src="https://github.com/yesic/pic/blob/master/dual/dual-13.jpg?raw=true" alt=""></p>]]></content>
    
    <summary type="html">
    
      &lt;blockquote class=&quot;blockquote-center&quot;&gt;&lt;p&gt;论文提出了基于注意力机制的两阶段循环神经网络（DA-RNN），在第一阶段（encoder），引入input attention mechanism对每一时刻的外部输入自适应性地提取相关性；在第二阶段（decoder），引入temporal attention mechanism捕获encoder的长期时序依赖信息。&lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
      <category term="paper notes" scheme="http://yoursite.com/categories/paper-notes/"/>
    
      <category term="IJCAI" scheme="http://yoursite.com/categories/paper-notes/IJCAI/"/>
    
      <category term="2017" scheme="http://yoursite.com/categories/paper-notes/IJCAI/2017/"/>
    
    
      <category term="attention" scheme="http://yoursite.com/tags/attention/"/>
    
      <category term="hierarchical" scheme="http://yoursite.com/tags/hierarchical/"/>
    
      <category term="encoder decoder" scheme="http://yoursite.com/tags/encoder-decoder/"/>
    
      <category term="time series prediction" scheme="http://yoursite.com/tags/time-series-prediction/"/>
    
  </entry>
  
</feed>
